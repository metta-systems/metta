== Metta architecture description ==

Metta is an exocortex OS: it is your external memory and social interaction agent. This is the primary task aside from
the additional multimedia and mobile assistant abilities. As an exocortex, OS's task is to provide means and ends to
sharing and gathering relevant information for you.

As an information gathering assistant the OS must support receiving, filtering, classifying and presenting relevant
information in a suitable form.

As an information sharing assistant the OS must support specifying privacy levels, distributing the secure storage,
publishing directly to your peers and to other media.

To provide this level of service, the key implementation principles are:

* It must be possible to distribute work and storage seamlessly between trusted nodes on the user's network.
* It must be possible to easily add and remove nodes from the trusted network.
* Information should be synchronized between nodes whenever possible.
* It should not be possible to bring down the node by overwhelming it with work: QoS guarantees must be agreed upon.
* Storage should be associative, not hierarchical. Cross-node links must be possible with full semantic relation data.
* Your data must be redundant, highly available and securely stored. "Publish once, share everywhere".
* Processing of multimedia and semantically meaningful data must be fast and lean.

Principles lead to technical requirements for the OS:

* QoS management on all levels in the system. Agents must be able to negotiate necessary resources and OS must maintain
promised resource guarantees.
* Highly componentised and sandboxed system. When executing agents on behalf of other users it is crucial to provide
data security for your own and for their data.
* Low system management overhead.


== Implementation details ==

Metta is implemented as a Single Address Space OS, for QoS management it uses technologies found in Nemesis OS,
part of Pegasus project from the University of Cambridge.

Implementation consists of kickstart, glue code, trusted computing base components, library components and applications.
Applications are vertically integrated - they perform most of the management themselves, which gives them high adaptivity
and provides QoS guarantees by removing contention in services usually done by the OS, which often become bottlenecks.

Common functionality is implemented as library components, which give standard tested implementations of many functions
needed by applications in Metta environment, and also save memory by sharing code and static data between applications.


== Single Address Space ==

Virtual address space in Metta is single, shared between all processes. This means that virtual to physical
mapping is equivalent in all domains.

One or more domains may be part of the same protection domain. This means they have the same access rights to the same
set of virtual address ranges.

Domain may be part of more than one protection domains. This allows easy sharing by specifying protection domains at the
stretch level and by adding domains using the same stretch into the same protection domain.

Implementation detail: on x86, for efficiency there is only one page directory and access rights for the memory frame
correspond to global stretch access rights. Once the domain is activated and tries to perform operation not allowed
by the global rights, it will fault and the fault handler will then check, if protection domain has more rights
actually, in this case the page directory will be updated with new rights and the stretch these frames belong to will
be added to a "altered stretches" list for the domain.

Any domain may request a stretch from a stretch allocator, specifying the desired size and (optionally) a starting address and attributes. Should the request be successful, a new stretch will be created and returned to the caller. The caller is now the owner of the stretch. The starting address and length of the returned stretch may then be queried; these will always be a multiple of the machine's page size.

Memory protection operations are carried out by the application through the stretch interface. This talks directly to the low-level translation system via simple system calls; it is not necessary to communicate with the system domain. Protection can be carried out in this way due to the protection model chosen which includes explicit rights for ``change permissions'' operations. A light-weight validation process checks if the caller is authorised to perform an operation.

It is also necessary that the frame which is being used for mapping (or which is being unmapped) is validated.
This involves ensuring that the calling domain owns the frame, and that the frame is not currently mapped or nailed.
These conditions are checked by using the RamTab, which is a simple enough structure to be used by low-level code.

Protection is carried out at stretch granularity -- every protection domain provides a mapping from the set of valid stretches to a subset of
read, write, execute, meta . A domain which holds the meta right is authorised to modify protections and mappings on the relevant stretch.

The accessibility of a stretch is determined by a combination of two things: the permissions for the stretch in the current protection domain and the global permissions of the stretch. The global permissions specify a minimum level of access that all domains share.

(Domain may grant read/write/execute to another domain, if needed.)

The translation system deals with inserting, retrieving or deleting mappings between virtual and physical addresses.
As such it may be considered an interface to a table of information held about these mappings; the actual mapping
will typically be performed as necessary by whatever memory management hardware or software is present.

The translation system is divided into two parts: a high-level management module, and the low-level trap handlers
and system calls. The high-level part is private to the system domain, and handles the following:

* Bootstrapping the `MMU' (in hardware or software), and setting up initial mappings.
* Adding, modifying or deleting ranges of virtual addresses, and performing the associated page table management.
* Creating and deleting protection domains.
* Initialising and partly maintaining the RamTab; this is a simple data structure maintaining information about
  the current use of frames of main memory.
  - ramtab - list of allocated physical frame ranges and their ownership and status

The high-level translation system is used by both the stretch allocator and the frames allocator.
The stretch allocator uses it to setup initial entries in the page table for stretches it has created,
or to remove such entries when a stretch is destroyed. These entries contain protection information but are
by default invalid: i.e. addresses within the range will cause a page fault if accessed.

The high-level part of the translation system is also in the system domain: this is machine-dependent code responsible
for the construction of page tables, and the setting up of NULL mappings for freshly allocated virtual addresses.
These mappings are used to hold the initial protection information, and by default are set up to cause a page fault
on the first access. Placing this functionality within the system domain means that the low-level translation system
does not need to be concerned with the allocation of page-table memory. It also allows protection faults,
page faults and ``unallocated address'' faults to be distinguished and dispatched to the faulting application.

==> initialize pagedir entries for allocated stretch

The frames allocator, on the other hand, uses the RamTab to record the owner and logical frame width
of allocated frames of main memory.

==> assign owned frames to appropriate domain.



Stretch driver is located inside application space, provided by the shared library code or implemented by the
application itself. It interfaces with frame allocator to provide backing RAM storage for stretches it manages.

User domain calls map/unmap from the stretch driver. Either mapping or unmapping a virtual address "va" requires that
the calling domain is executing in a protection domain which holds a meta right for the stretch containing "va".
A consequence of this is that it is not possible to map a virtual address which is not part of some stretch.

== Stretch Allocator: implementation details ==

stretch_allocator [privileged]
User API via: stretch_allocator_v1::allocate_stretch()/release_stretch()
Kernel impl: protection_domain_t
- range lists of available virtual memory ranges
+ allocate_stretch
+ release_stretch
* there's a VA_quota implied on all stretches owned by a domain

frame_allocator [privileged]
User API via: frame_allocator_v1
Kernel impl: frame_allocator_t
- range lists of available physical memory ranges
+ allocate_frame
+ free_frame
* There's a frame quota implied on a domain

protection_domain (via stretch_allocator?) [kernel]
- stretches list
+ get stretch for given va?

stretch_driver (bound to stretch) [userspace]
1. map(va, pa, attr) : arrange that the virtual address va maps onto the physical address pa with the (machine-dependent) PTE attributes attr.
2. unmap(va) : remove the mapping of the virtual address va. Any further access to the address should cause some form of memory fault.
3. mapping(va) ->  (pa, attr) : retrieve the current mapping of the virtual address va, if any.
--> frame_allocator::allocate_region (uses constraints to allocate specific physmem for stretch)
--> routed to prot_dom mapping api? via syscalls


DRM/GEM memory management details in Linux: http://lwn.net/Articles/283798/


== Kickstart ==

Kickstart does all preinitialization work needed to get system going - it creates a system privileged domain, initializes
the MMU and boots processors, then gives control to the boot modules loader. Module loader will resolve module dependencies,
determine load order and load the TCB modules â€” these are the most important system modules, always trusted by other
components.


== Glue code ==

Glue code performs only a minimally necessary subset of operations that require privileged CPU mode. This includes
manipulating MMU tables and switching protection domains. This code is therefore not preemptible.

Code exists in glue layer for a number of reasons:
- code is privileged and therefore needs to execute with supervisor permissions,
- code is executed because of exception or interrupt and therefore needs to be both privileged and uninterruptible,
- code is generally used and needs to run atomically/uninterruptible.

Interrupts are also implemented as stubs in glue code, due to their privileged nature.

Some glue code syscalls are privileged and can be used only by members of the TCB, others are used by application
processes to request work from other domains.


== Trusted Computing Base ==

TCB components implement features absolutely necessary for application functioning and therefore define the OS kernel.

Kernel components have almost no private data, on which contention could arise. Most of the data for kernel calls
is provided by the process engaged in the syscall, therefore not affecting service of other processes.
This also helps API atomicity.

Components export functionality through one or more interfaces. Kernel and userspace components are accessed via
interfaces alike.

All interfaces are strongly typed, and these types are defined in an interface definition language. It is clearly
important, therefore, to start with a good type system, and [Evers 93] presents a good discussion of the issues of
typing in a systems environment. The type system used in Metta is a hybrid: it includes notions both of the abstract
types of interfaces and of concrete data types. It represents a compromise between the conceptual elegance and
software engineering benefits of purely abstract type systems such as that used in Emerald [Raj 91], and the
requirements of efficiency and inter-operability: the goal is to implement an operating system with few restrictions
on programming language.

Concrete types are data types whose structure is explicit. They can be predefined (such as booleans, strings, and
integers of various sizes) or constructed (as with records, arrays, etc). The space of concrete types also includes
typed references to interfaces.

Interfaces are instances of ADTs. Interfaces are rarely static: they can be dynamically created and references to them
passed around freely. The type system includes a simple concept of subtyping. An interface type can be a subtype of
another ADT, in which case it supports all the operations of the supertype, and an instance of the subtype can be used
where an instance of the supertype is required.

The operations supported by interfaces are like procedure calls: they take a number of arguments and normally return
a number of results. They can also raise exceptions, which themselves can take arguments.

An interface definition language is used to specify the types, exceptions and methods of an interface,
and a run-time typesystem allows the narrowing of types and the marshaling of parameters for non-local procedure
invocations.

A name-space scheme (based on Plan-9 contexts) allows implementations of interfaces to be published and a trader
component from the TCB may be used to find component(s) exporting a particular interface type or instance.

There are few restrictions on how the name space is structured. The model followed is that of [Saltzer 79]:
a name is a textual string, a binding is an association of a name with some value, and a context is a collection of
bindings. Resolving a name is the process of locating the value bound to it. Name resolution requires that a context
be specified.


== Library Components ==

Library components define the base substrate upon which the whole applications are built. Library components are
real components and they export typed interfaces just like any other component does. Most library components are
colocated into the same protection domain as the application using them.

Dynamic loader (Sjofn), similar to OMOS server, is used to perform component relocation and linking. Employed memory
and loading models allow to share code and static data between all domains efficiently.

Meta-objects (in OMOS sense) are used to create generator interfaces which instantiate modules, used by application.


== Applications ==

Applications consist of standard and custom components, which provide most of the functionality and main driver code.
Applications service themselves - i.e. they service their own page faults or CPU scheduling decisions, often using
standard components that provide necessary functionality already.

After startup application receives it's own domain, protection domain, initially not shared with any other domains
and a set of pervasives - a naming context which it can use to find other necessary components, a virtual CPU interface
which it can use to make scheduling decisions, a stretch and heap interfaces which it can use to allocate memory.


== VCPU ==

Virtual CPU interface allows domains to receive notifications when they are granted or revoked a CPU time slice or when
a page fault or some other exception occurs.

VCPUs indeed perform inheritance scheduling - activation handler usually calls a scheduler function, which chooses next
thread to run, by giving it the rest of CPU tick.


== IDC ==

The only default mean of inter-domain communication is Event. Sending an event is a relatively lightweight operation,
upon which many other syncronization and communication primitives may be built.

A networked IDC is also possible, given that component interfaces are generated from the IDL files, and therefore
can provide marshalling and unmarshalling information for instantiating component interface stubs.

Portals - sections of JIT generated code, that performs argument marshalling and request PD switch. Portals are
generated by portal manager, which retrieves portal specifications from Interface Definition files. Portal manager
is free to perform some specific optimizations, like portal short-circuiting, which in some cases can save a network
roundtrip.


== Kickstart: sequence ==

This part starts in protected mode, linear addresses equal physical, paging is off.

- set up 1:1 page mapping for the bottom 4Mb of RAM
- map glue and boot components to their designated addresses (starting, say at 4Mb)
- enter paged mode
- init glue code
  - initialize exception and interrupt handlers
  - init syscalls interface page
- init linker
- find bootimage PCBs
- run PCB initialization upcalls
  - component constructors will run in kernel mode, have the ability to set up their system tables etcetc,
- enter them into schedule
- run scheduler

bootloader
+---kickstart
    +---setup initial page mappings
    +---map/move glue and boot components to their designated addresses
    +---enable paging
    +---glue_init
        +---initialize syscalls interface page
    +---kernel_init
        +---initialize exception and interrupt handlers
        +---find bootimage PCBs
        +---run PCB initialization upcalls
        +---enter them into schedule
        +---run scheduler



 * glue code:
 * - exception handlers/stubs
 * - minimal privileged and uninterruptible code
 * - syscalls interface page

bootimage:

kernel: kickstart
module: initfs index | glue | boot TCBs

for simplicity, prelink initial components at fixed addresses and put them into normal ELF files. store entry point
address from ELF file to PCB using buildpcb tool.


Physical memory layout during kickstart:

          physical memory
      |                      |
      | .................... |
 1Mb  +----------------------+ 0x0010_0000 --------> 0x0010_0000 identity mapped
      | kickstart code       |
      +----------------------+ 0x0010_7000
      | page directory       |
      +----------------------+ 0x0010_8000
      | kickstart data       |
      +----------------------+ 0x0010_9000 --------> not mapped
      |                      |
      | kernel module        |
      |                      |
      +----------------------+ 0x0011_1000
      |                      |
      | bootcp module        |
      |                      |
      +----------------------+ 0x0011_f000 --------> placement alloc starts here
      | bootinfo page        |
      +----------------------+ 0x0012_0000
      | pagetable            |
      +----------------------+ 0x0012_1000 --------> 0xf000_0000 higher-half mapped
      | nucleus code         |
      +----------------------+ 0x0012_2000
      | pagetable            |
      +----------------------+ 0x0012_3000 --------> 0xf000_1000
      | nucleus code         |
      +----------------------+ 0x0012_4000 --------> 0xf000_2000
      | nucleus code         |
      +----------------------+ 0x0012_5000 --------> 0xf000_3000
      | nucleus code         |
      +----------------------+ 0x0012_6000 --------> 0xf000_4000
      | nucleus data         |
      +----------------------+ 0x0012_7000 --------> 0xf000_5000
      | nucleus data         |
      +----------------------+ 0x0012_8000
      |                      |
      | .................... |
      |                      |


== Glue code: syscalls ==

Available Nemesis syscalls to glue code:

  privileged:
    ntsc_swpipl        // Change interrupt priority level.
    ntsc_entkern       // Enter kernel mode.
    ntsc_leavekern     // Leave kernel mode.
    ntsc_kevent        // Send an event from an interrupt stub.
    ntsc_rti           // Return from an interrupt stub.

Syscalls for metta glue:

  privileged:
    ??sc_activate(vcpu_t*)  Activate a process

  unprivileged:
    sc_return()    Return from activation, reentrancy reenabled and code resumed from the next instruction after call.
    sc_return_resume(context_t*) Return from activation, resuming passed context.
    sc_return_block()  Return from activation and block.
    sc_block()         Block awaiting an event.
    sc_yield()         Relinquish CPU allocation for this period.
    sc_send(int n, uint64_t event)      Send an event.


== Applications: startup ==

From the starting application process viewpoint:

- load application code and data
- read library dependencies
- for missing libraries, load them and their dependencies
- link calls from app to used libraries


Library viewpoint:
- library implements a component interface
- this means per-client data is allocated by calling constructors of the interface
- libraries which do not have per-client data may implement interfaces directly, but i presume this is rare.

A typical application:
- load trader library
- load memory library
- instantiate trader interface
- instantiate memory interface
- allocate something
- use trader to locate a peer
- instantiate a peer interface
- send something to peer via interface instance


--------8<--------imaginary cut line--------8<--------do not cross-------->8--------imaginary cut line-------->8--------

To construct portable kernel, at least 3 different architectures should be supported.

* x86
* arm
* hosted

Pistachio has an excellent c++ implementation of the kernel - use it as reference.
(e.g. OSdev/L4/l4ka-pistachio/kernel/src/generic)

Kernel consists of minimal glue code, providing exception and interrupt handling.


Answer these questions first:

* Q: how do VCPUs get assigned between domains?

A: "A privileged service called the Domain Manager creates DCBs and links them into the scheduler data structures."

Glue code contains scheduler (EDF) to schedule domains: "glue -> scheduler -> VCPU -> domain"

Glue code contains only: domain scheduler, a few system calls (6), interrupt dispatcher support, timer interrupt
for scheduler, startup routines, minimal console for emergency output.





*The kernel's view of a domain is limited to a single data structure called the Domain Control Block*

domain startup: closure(methods: Go; state: everything new domain needs to get going)



Components are pieces of code and static data (which has no mutable state).
(Clemens Szypersky defines a software component as a unit of composition with contractually specified
interfaces and explicit context dependencies only.)

Applications are built from interconnected components.

Components link from several shared libraries. Libraries are shared across whole system and provide means to save
code size by sharing common implementations across running applications. Components are designed to offer services
through interfaces, which embed references to current (per-thread) component state and list of methods that could be
called through this interface.

in-component dependencies: symbols and objects needed to build together component object.
inter-component dependencies: other interfaces that are needed for functioning of this component, can be fullfilled
by building or finding other component objects which provide such interfaces.


[Component1: file.c other.o library.so [exported interfaces: Interface1 Interface2] [required interfaces: FileIO]]

here, library.so's code and static data can be shared between multiple components.

Q: how to create proper component instance state if library.so needs to have some local data too?



For efficient linking we use technique used by OMOS object/meta-object server.
